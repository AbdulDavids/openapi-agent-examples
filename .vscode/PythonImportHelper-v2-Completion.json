[
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "haystack",
        "description": "haystack",
        "isExtraImport": true,
        "detail": "haystack",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "haystack.dataclasses",
        "description": "haystack.dataclasses",
        "isExtraImport": true,
        "detail": "haystack.dataclasses",
        "documentation": {}
    },
    {
        "label": "ChatPromptBuilder",
        "importPath": "haystack.components.builders",
        "description": "haystack.components.builders",
        "isExtraImport": true,
        "detail": "haystack.components.builders",
        "documentation": {}
    },
    {
        "label": "OpenAIChatGenerator",
        "importPath": "haystack.components.generators.chat",
        "description": "haystack.components.generators.chat",
        "isExtraImport": true,
        "detail": "haystack.components.generators.chat",
        "documentation": {}
    },
    {
        "label": "OpenAPITool",
        "importPath": "haystack_experimental.components.tools.openapi",
        "description": "haystack_experimental.components.tools.openapi",
        "isExtraImport": true,
        "detail": "haystack_experimental.components.tools.openapi",
        "documentation": {}
    },
    {
        "label": "LLMProvider",
        "importPath": "haystack_experimental.components.tools.openapi",
        "description": "haystack_experimental.components.tools.openapi",
        "isExtraImport": true,
        "detail": "haystack_experimental.components.tools.openapi",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "RequestsWrapper",
        "importPath": "langchain_community.utilities.requests",
        "description": "langchain_community.utilities.requests",
        "isExtraImport": true,
        "detail": "langchain_community.utilities.requests",
        "documentation": {}
    },
    {
        "label": "planner",
        "importPath": "langchain_community.agent_toolkits.openapi",
        "description": "langchain_community.agent_toolkits.openapi",
        "isExtraImport": true,
        "detail": "langchain_community.agent_toolkits.openapi",
        "documentation": {}
    },
    {
        "label": "reduce_openapi_spec",
        "importPath": "langchain_community.agent_toolkits.openapi.spec",
        "description": "langchain_community.agent_toolkits.openapi.spec",
        "isExtraImport": true,
        "detail": "langchain_community.agent_toolkits.openapi.spec",
        "documentation": {}
    },
    {
        "label": "ChatAnthropic",
        "importPath": "langchain_anthropic",
        "description": "langchain_anthropic",
        "isExtraImport": true,
        "detail": "langchain_anthropic",
        "documentation": {}
    },
    {
        "label": "RaceWinner",
        "kind": 6,
        "importPath": "f1-fastapi-server.main",
        "description": "f1-fastapi-server.main",
        "peekOfCode": "class RaceWinner(BaseModel):\n    race: str\n    winner: str\n    class Config:\n        schema_extra = {\n            \"example\": {\"race\": \"bahrain\", \"winner\": \"Lewis Hamilton\"},\n        }\nclass YearWinnersResponse(BaseModel):\n    year: int\n    winners: List[RaceWinner]",
        "detail": "f1-fastapi-server.main",
        "documentation": {}
    },
    {
        "label": "YearWinnersResponse",
        "kind": 6,
        "importPath": "f1-fastapi-server.main",
        "description": "f1-fastapi-server.main",
        "peekOfCode": "class YearWinnersResponse(BaseModel):\n    year: int\n    winners: List[RaceWinner]\n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"year\": 2021,\n                \"winners\": [\n                    {\"race\": \"bahrain\", \"winner\": \"Lewis Hamilton\"},\n                    {\"race\": \"emilia_romagna\", \"winner\": \"Max Verstappen\"},",
        "detail": "f1-fastapi-server.main",
        "documentation": {}
    },
    {
        "label": "RaceWinnerResponse",
        "kind": 6,
        "importPath": "f1-fastapi-server.main",
        "description": "f1-fastapi-server.main",
        "peekOfCode": "class RaceWinnerResponse(BaseModel):\n    year: int\n    race: str\n    winner: str\n    class Config:\n        schema_extra = {\n            \"example\": {\"race\": \"bahrain\", \"winner\": \"Lewis Hamilton\"}\n        }\n# Root endpoint\n@app.get(\"/\")",
        "detail": "f1-fastapi-server.main",
        "documentation": {}
    },
    {
        "label": "read_root",
        "kind": 2,
        "importPath": "f1-fastapi-server.main",
        "description": "f1-fastapi-server.main",
        "peekOfCode": "def read_root():\n    return {\"message\": \"Welcome to the F1 Race Winners API!\"}\n# Endpoint to get winners for a specific year\n@app.get(\"/winners/{year}\", response_model=YearWinnersResponse)\ndef get_year_winners(year: int):\n    if year in f1_race_winners:\n        return {\"year\": year, \"winners\": f1_race_winners[year]}\n    return {\"error\": \"Data not available for the requested year.\"}\n# Endpoint to get winners for a specific year and race\n@app.get(\"/winners/{year}/{race}\", response_model=RaceWinnerResponse)",
        "detail": "f1-fastapi-server.main",
        "documentation": {}
    },
    {
        "label": "get_year_winners",
        "kind": 2,
        "importPath": "f1-fastapi-server.main",
        "description": "f1-fastapi-server.main",
        "peekOfCode": "def get_year_winners(year: int):\n    if year in f1_race_winners:\n        return {\"year\": year, \"winners\": f1_race_winners[year]}\n    return {\"error\": \"Data not available for the requested year.\"}\n# Endpoint to get winners for a specific year and race\n@app.get(\"/winners/{year}/{race}\", response_model=RaceWinnerResponse)\ndef get_race_winner(year: int, race : str):\n    if year in f1_race_winners:\n        for race_info in f1_race_winners[year]:\n            if race_info[\"race\"].strip().lower() == race.strip().lower():",
        "detail": "f1-fastapi-server.main",
        "documentation": {}
    },
    {
        "label": "get_race_winner",
        "kind": 2,
        "importPath": "f1-fastapi-server.main",
        "description": "f1-fastapi-server.main",
        "peekOfCode": "def get_race_winner(year: int, race : str):\n    if year in f1_race_winners:\n        for race_info in f1_race_winners[year]:\n            if race_info[\"race\"].strip().lower() == race.strip().lower():\n                return {\"year\": year, \"race\": race, \"winner\": race_info[\"winner\"]}\n        return {\"error\": \"Race not found for the requested year.\"}\n    return {\"error\": \"Data not available for the requested year.\"}",
        "detail": "f1-fastapi-server.main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "f1-fastapi-server.main",
        "description": "f1-fastapi-server.main",
        "peekOfCode": "app = FastAPI(servers=[{\"url\": \"http://localhost:8000\"}])\n# Fake data of F1 race winners\nf1_race_winners = {\n    2021: [\n        {\"race\": \"bahrain\", \"winner\": \"Lewis Hamilton\"},\n        {\"race\": \"emilia_romagna\", \"winner\": \"Max Verstappen\"},\n        {\"race\": \"portuguese\", \"winner\": \"Lewis Hamilton\"},\n    ],\n    2022: [\n        {\"race\": \"bahrain\", \"winner\": \"Charles Leclerc\"},",
        "detail": "f1-fastapi-server.main",
        "documentation": {}
    },
    {
        "label": "f1_race_winners",
        "kind": 5,
        "importPath": "f1-fastapi-server.main",
        "description": "f1-fastapi-server.main",
        "peekOfCode": "f1_race_winners = {\n    2021: [\n        {\"race\": \"bahrain\", \"winner\": \"Lewis Hamilton\"},\n        {\"race\": \"emilia_romagna\", \"winner\": \"Max Verstappen\"},\n        {\"race\": \"portuguese\", \"winner\": \"Lewis Hamilton\"},\n    ],\n    2022: [\n        {\"race\": \"bahrain\", \"winner\": \"Charles Leclerc\"},\n        {\"race\": \"saudi_arabia\", \"winner\": \"Max Verstappen\"},\n        {\"race\": \"australian\", \"winner\": \"Charles Leclerc\"},",
        "detail": "f1-fastapi-server.main",
        "documentation": {}
    },
    {
        "label": "query_f1_pipeline",
        "kind": 2,
        "importPath": "haystack-agent.haystack_agent",
        "description": "haystack-agent.haystack_agent",
        "peekOfCode": "def query_f1_pipeline(user_query: str):\n    \"\"\"\n    Run the F1 bot pipeline with the user's query.\n    :param user_query: The user's query as a string.\n    :return: The response generated by the LLM.\n    \"\"\"\n    result = pipe.run(data={\n        \"f1_tool\": {\n            \"messages\": [ChatMessage.from_user(user_query)]\n        },",
        "detail": "haystack-agent.haystack_agent",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "haystack-agent.haystack_agent",
        "description": "haystack-agent.haystack_agent",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"Query the F1 pipeline CLI tool.\")\n    parser.add_argument(\"query\", type=str, help=\"User query. E.g: 'Who won in Monaco in 2024?'\")\n    parser.add_argument(\"--model\", type=str, default=\"gpt-4\", help=\"Model name\")\n    parser.add_argument(\"--timeout\", type=int, default=10, help=\"Timeout in seconds\")\n    parser.add_argument(\"--max_tokens\", type=int, default=1024, help=\"Maximum tokens for response\")\n    args = parser.parse_args()\n    # Update LLM configuration with arguments\n    llm.generation_kwargs[\"max_tokens\"] = args.max_tokens\n    # Process the query",
        "detail": "haystack-agent.haystack_agent",
        "documentation": {}
    },
    {
        "label": "ALLOW_DANGEROUS_REQUEST",
        "kind": 5,
        "importPath": "haystack-agent.haystack_agent",
        "description": "haystack-agent.haystack_agent",
        "peekOfCode": "ALLOW_DANGEROUS_REQUEST = True\n# Initialize the OpenAPI tool with the F1 API schema\nf1_tool = OpenAPITool(\n    generator_api=LLMProvider.OPENAI,\n    spec=\"openapi.yaml\",\n)\n# Define the prompt\nmessages = [\n    ChatMessage.from_system(\"Answer the F1 query using the API. Race names with two words should be separated by an underscore and be in lowercase. The API stores data from 2021 to 2024.\"),\n    ChatMessage.from_user(\"User asked: {{user_message}}\"),",
        "detail": "haystack-agent.haystack_agent",
        "documentation": {}
    },
    {
        "label": "f1_tool",
        "kind": 5,
        "importPath": "haystack-agent.haystack_agent",
        "description": "haystack-agent.haystack_agent",
        "peekOfCode": "f1_tool = OpenAPITool(\n    generator_api=LLMProvider.OPENAI,\n    spec=\"openapi.yaml\",\n)\n# Define the prompt\nmessages = [\n    ChatMessage.from_system(\"Answer the F1 query using the API. Race names with two words should be separated by an underscore and be in lowercase. The API stores data from 2021 to 2024.\"),\n    ChatMessage.from_user(\"User asked: {{user_message}}\"),\n    ChatMessage.from_system(\"API responded: {{service_response}}\")\n]",
        "detail": "haystack-agent.haystack_agent",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "haystack-agent.haystack_agent",
        "description": "haystack-agent.haystack_agent",
        "peekOfCode": "messages = [\n    ChatMessage.from_system(\"Answer the F1 query using the API. Race names with two words should be separated by an underscore and be in lowercase. The API stores data from 2021 to 2024.\"),\n    ChatMessage.from_user(\"User asked: {{user_message}}\"),\n    ChatMessage.from_system(\"API responded: {{service_response}}\")\n]\nbuilder = ChatPromptBuilder(template=messages)\n# Initialize the LLM\nllm = OpenAIChatGenerator(generation_kwargs={\"max_tokens\": 1024})\n# Define the pipeline components\npipe = Pipeline()",
        "detail": "haystack-agent.haystack_agent",
        "documentation": {}
    },
    {
        "label": "builder",
        "kind": 5,
        "importPath": "haystack-agent.haystack_agent",
        "description": "haystack-agent.haystack_agent",
        "peekOfCode": "builder = ChatPromptBuilder(template=messages)\n# Initialize the LLM\nllm = OpenAIChatGenerator(generation_kwargs={\"max_tokens\": 1024})\n# Define the pipeline components\npipe = Pipeline()\npipe.add_component(\"f1_tool\", f1_tool)\npipe.add_component(\"builder\", builder)\npipe.add_component(\"llm\", llm)\n# Connect the pipeline components\npipe.connect(\"f1_tool.service_response\", \"builder.service_response\")",
        "detail": "haystack-agent.haystack_agent",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "haystack-agent.haystack_agent",
        "description": "haystack-agent.haystack_agent",
        "peekOfCode": "llm = OpenAIChatGenerator(generation_kwargs={\"max_tokens\": 1024})\n# Define the pipeline components\npipe = Pipeline()\npipe.add_component(\"f1_tool\", f1_tool)\npipe.add_component(\"builder\", builder)\npipe.add_component(\"llm\", llm)\n# Connect the pipeline components\npipe.connect(\"f1_tool.service_response\", \"builder.service_response\")\npipe.connect(\"builder.prompt\", \"llm.messages\")\n# Run the pipeline with a question",
        "detail": "haystack-agent.haystack_agent",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "haystack-agent.haystack_agent",
        "description": "haystack-agent.haystack_agent",
        "peekOfCode": "pipe = Pipeline()\npipe.add_component(\"f1_tool\", f1_tool)\npipe.add_component(\"builder\", builder)\npipe.add_component(\"llm\", llm)\n# Connect the pipeline components\npipe.connect(\"f1_tool.service_response\", \"builder.service_response\")\npipe.connect(\"builder.prompt\", \"llm.messages\")\n# Run the pipeline with a question\ndef query_f1_pipeline(user_query: str):\n    \"\"\"",
        "detail": "haystack-agent.haystack_agent",
        "documentation": {}
    },
    {
        "label": "ALLOW_DANGEROUS_REQUEST",
        "kind": 5,
        "importPath": "langchain-agent.langchain_agent",
        "description": "langchain-agent.langchain_agent",
        "peekOfCode": "ALLOW_DANGEROUS_REQUEST = True\n# Make sure we have the Anthropic API key environment variable set\nif \"ANTHROPIC_API_KEY\" not in os.environ:\n    raise ValueError(\"ANTHROPIC_API_KEY environment variable not set\")\nargparser = argparse.ArgumentParser()\nargparser.add_argument(\n    \"query\", type=str, help=\"User query. E.g: 'Who won in Monaco in 2024?'\"\n)\nargparser.add_argument(\n    \"--model\", type=str, default=\"claude-3-sonnet-20240229\", help=\"Model name\"",
        "detail": "langchain-agent.langchain_agent",
        "documentation": {}
    },
    {
        "label": "argparser",
        "kind": 5,
        "importPath": "langchain-agent.langchain_agent",
        "description": "langchain-agent.langchain_agent",
        "peekOfCode": "argparser = argparse.ArgumentParser()\nargparser.add_argument(\n    \"query\", type=str, help=\"User query. E.g: 'Who won in Monaco in 2024?'\"\n)\nargparser.add_argument(\n    \"--model\", type=str, default=\"claude-3-sonnet-20240229\", help=\"Model name\"\n)\nargparser.add_argument(\"--timeout\", type=int, default=10, help=\"Timeout in seconds\")\nargparser.add_argument(\"--stop\", type=str, default=\"</s>\", help=\"Stop token\")\nargs = argparser.parse_args()",
        "detail": "langchain-agent.langchain_agent",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "langchain-agent.langchain_agent",
        "description": "langchain-agent.langchain_agent",
        "peekOfCode": "args = argparser.parse_args()\nuser_query = (args.query,)\nmodel = ChatAnthropic(\n    model_name=args.model,\n    timeout=args.timeout,\n    stop=[\n        args.stop,\n    ],\n)\nwith open(\"openapi.json\") as f:",
        "detail": "langchain-agent.langchain_agent",
        "documentation": {}
    },
    {
        "label": "user_query",
        "kind": 5,
        "importPath": "langchain-agent.langchain_agent",
        "description": "langchain-agent.langchain_agent",
        "peekOfCode": "user_query = (args.query,)\nmodel = ChatAnthropic(\n    model_name=args.model,\n    timeout=args.timeout,\n    stop=[\n        args.stop,\n    ],\n)\nwith open(\"openapi.json\") as f:\n    openapi = json.load(f)",
        "detail": "langchain-agent.langchain_agent",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "langchain-agent.langchain_agent",
        "description": "langchain-agent.langchain_agent",
        "peekOfCode": "model = ChatAnthropic(\n    model_name=args.model,\n    timeout=args.timeout,\n    stop=[\n        args.stop,\n    ],\n)\nwith open(\"openapi.json\") as f:\n    openapi = json.load(f)\nrequests_wrapper = RequestsWrapper()",
        "detail": "langchain-agent.langchain_agent",
        "documentation": {}
    },
    {
        "label": "requests_wrapper",
        "kind": 5,
        "importPath": "langchain-agent.langchain_agent",
        "description": "langchain-agent.langchain_agent",
        "peekOfCode": "requests_wrapper = RequestsWrapper()\nf1_spec = reduce_openapi_spec(openapi)\nf1_agent = planner.create_openapi_agent(\n    f1_spec, requests_wrapper, model, allow_dangerous_requests=ALLOW_DANGEROUS_REQUEST\n)\nmessages = [  \n(\"system\", \"Answer the F1 query using the API. Race names with two words should be separated by an underscore and be in lowercase. The API stores data from 2021 to 2024.\"),  \n(\"human\", user_query),  \n]  \nf1_agent.invoke(messages)",
        "detail": "langchain-agent.langchain_agent",
        "documentation": {}
    },
    {
        "label": "f1_spec",
        "kind": 5,
        "importPath": "langchain-agent.langchain_agent",
        "description": "langchain-agent.langchain_agent",
        "peekOfCode": "f1_spec = reduce_openapi_spec(openapi)\nf1_agent = planner.create_openapi_agent(\n    f1_spec, requests_wrapper, model, allow_dangerous_requests=ALLOW_DANGEROUS_REQUEST\n)\nmessages = [  \n(\"system\", \"Answer the F1 query using the API. Race names with two words should be separated by an underscore and be in lowercase. The API stores data from 2021 to 2024.\"),  \n(\"human\", user_query),  \n]  \nf1_agent.invoke(messages)",
        "detail": "langchain-agent.langchain_agent",
        "documentation": {}
    },
    {
        "label": "f1_agent",
        "kind": 5,
        "importPath": "langchain-agent.langchain_agent",
        "description": "langchain-agent.langchain_agent",
        "peekOfCode": "f1_agent = planner.create_openapi_agent(\n    f1_spec, requests_wrapper, model, allow_dangerous_requests=ALLOW_DANGEROUS_REQUEST\n)\nmessages = [  \n(\"system\", \"Answer the F1 query using the API. Race names with two words should be separated by an underscore and be in lowercase. The API stores data from 2021 to 2024.\"),  \n(\"human\", user_query),  \n]  \nf1_agent.invoke(messages)",
        "detail": "langchain-agent.langchain_agent",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "langchain-agent.langchain_agent",
        "description": "langchain-agent.langchain_agent",
        "peekOfCode": "messages = [  \n(\"system\", \"Answer the F1 query using the API. Race names with two words should be separated by an underscore and be in lowercase. The API stores data from 2021 to 2024.\"),  \n(\"human\", user_query),  \n]  \nf1_agent.invoke(messages)",
        "detail": "langchain-agent.langchain_agent",
        "documentation": {}
    }
]